# ğŸ™ï¸ Speech Emotion Recognition using ANN  

## ğŸ“Œ Project Overview  
This project focuses on **Speech Emotion Recognition (SER)** using an **Artificial Neural Network (ANN)**.  
The system classifies human emotions (such as *happy, sad, angry, fearful, etc.*) from speech signals by extracting relevant audio features.  

It leverages the **RAVDESS dataset** for training and uses **Librosa** for feature extraction, **TensorFlow/Keras** for deep learning, and **Gradio** for deployment.  

---

## âš¡ Key Features  
- ğŸµ **Audio Feature Extraction**: MFCCs, Chroma, Zero Crossing Rate, Spectral Contrast, and Tonnetz  
- ğŸ§  **Deep Learning Model**: Artificial Neural Network (ANN) with TensorFlow/Keras  
- ğŸ“Š **Model Performance**: Achieved **54%+ test accuracy** on emotion classification  
- ğŸŒ **Deployment**: Interactive Gradio interface for real-time emotion recognition  
- ğŸ“‚ **Dataset**: RAVDESS Emotional Speech Dataset  

---

## ğŸ› ï¸ Tech Stack  
- **Programming Language**: Python  
- **Libraries**: TensorFlow, Keras, Scikit-learn, Librosa, Pandas, NumPy, Matplotlib, Seaborn, Gradio  
- **Tools**: Jupyter Notebook, GitHub  

---

## ğŸš€ How to Run  

1. **Clone the repository**  
   ```bash
   git clone https://github.com/your-username/speech-emotion-recognition.git
   cd speech-emotion-recognition

2. **Install dependencies** 
     ```bash
     pip install -r requirements.txt

3. **Run the notebook for training** 
     ```bash
     jupyter notebook SpeechEmotionRecognition.ipynb


## ğŸ“‚ Project Structure
 ```bash
speech-emotion-recognition/
â”‚-- data/                  # Dataset (RAVDESS)
â”‚-- models/                # Saved models
â”‚-- SpeechEmotionRecognition.ipynb  # Training Notebook
â”‚-- app.py                 # Gradio app for deployment
â”‚-- requirements.txt       # Dependencies
â”‚-- README.md              # Project documentation
 ```

## ğŸ”§ Configuration

- Dataset: RAVDESS
- Sample rate: 22050 Hz
- Features extracted: MFCCs, Chroma, ZCR, Spectral Contrast, Tonnetz

## ğŸ§ª Testing

- Tested on RAVDESS dataset (speech subset)
- Achieved 54%+ accuracy on test set

## ğŸ“Š Results

- Achieved 54%+ test accuracy on the RAVDESS dataset
- Deployed an interactive Gradio app for real-time voice emotion detection

## ğŸ‘¨â€ğŸ’» Authors / Credits

- Developed by Shrunkhala Sisodia
- ğŸ“ Masterâ€™s in Business Intelligence & Analytics
- ğŸŒ Location: Ahmedabad, India
- ğŸ”—[LinkedIn](https://www.linkedin.com/in/shrunkhalasandeepsisodia/)
- Dataset: RAVDESS

## ğŸ“Œ Future Improvements

- Improve accuracy by experimenting with CNN/LSTM models
- Test on larger and diverse datasets
- Integrate with a real-time speech recognition pipeline 

