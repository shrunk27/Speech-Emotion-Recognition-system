# ğŸ™ï¸ Speech Emotion Recognition using ANN  

## ğŸ“Œ Project Overview  
This project focuses on **Speech Emotion Recognition (SER)** using an **Artificial Neural Network (ANN)**.  
The system classifies human emotions (such as *happy, sad, angry, fearful, etc.*) from speech signals by extracting relevant audio features.  

It leverages the **RAVDESS dataset** for training and uses **Librosa** for feature extraction, **TensorFlow/Keras** for deep learning, and **Gradio** for deployment.  

---

## âš¡ Key Features  
- ğŸµ **Audio Feature Extraction**: MFCCs, Chroma, Zero Crossing Rate, Spectral Contrast, and Tonnetz  
- ğŸ§  **Deep Learning Model**: Artificial Neural Network (ANN) with TensorFlow/Keras  
- ğŸ“Š **Model Performance**: Achieved **54%+ test accuracy** on emotion classification  
- ğŸŒ **Deployment**: Interactive Gradio interface for real-time emotion recognition  
- ğŸ“‚ **Dataset**: RAVDESS Emotional Speech Dataset  

---

## ğŸ› ï¸ Tech Stack  
- **Programming Language**: Python  
- **Libraries**: TensorFlow, Keras, Scikit-learn, Librosa, Pandas, NumPy, Matplotlib, Seaborn, Gradio  
- **Tools**: Jupyter Notebook, GitHub  

---

## ğŸš€ How to Run  

1. **Clone the repository**  
   ```bash
   git clone https://github.com/your-username/speech-emotion-recognition.git
   cd speech-emotion-recognition

2. **Install dependencies** 
    pip install -r requirements.txt

3. **Run the notebook for training** 
    jupyter notebook SpeechEmotionRecognition.ipynb

## ğŸ“Š Results

- Achieved 54%+ test accuracy on the RAVDESS dataset
- Deployed an interactive Gradio app for real-time voice emotion detection

## ğŸ“Œ Future Improvements

- Improve accuracy by experimenting with CNN/LSTM models
- Test on larger and diverse datasets
- Integrate with a real-time speech recognition pipeline 

